{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# PEC 3: Noviembre 2019\n",
    "\n",
    "## Extracción de conocimiento de fuentes de datos heterogéneas mediante Spark SQL, RDDs y GraphFrames\n",
    "\n",
    "En esta práctica vamos a introducir estructuras de datos más complejas que las vistas hasta ahora, donde los campos pueden a su vez tener campos anidados. En concreto utilizaremos datos de twitter capturados en el contexto de las elecciones generales en España del 28 de Abril de 2019. La práctica está estructurada de la siguiente manera:\n",
    "- **Parte 0:** Configuración del entorno\n",
    "- **Parte 1:** Introducción a data frames estructurados y cómo operar extraer información *(1.5 puntos)*\n",
    "    - **Parte 1.1:** Importar los datos *(0.25 puntos)*\n",
    "    - **Parte 1.2:** *Queries* sobre sobre data frames complejos *(1.25 puntos)*\n",
    "        - **Parte 1.2.1:** Queries SQL *(0.5 puntos)*\n",
    "        - **Parte 1.2.2:** Queries sobre el pipeline *(0.75 puntos)*\n",
    "- **Parte 2:** Bases de datos HIVE y operaciones complejas *(3.5 puntos)*\n",
    "    - **Parte 2.1:** Bases de datos Hive *(0.25 puntos)*\n",
    "    - **Parte 2.2:** Más allá de las transformaciones SQL *(2.75 puntos)*\n",
    "        - **Parte 2.2.1:** Tweets por población  *(1.25 puntos)*\n",
    "            - **Parte 2.2.1.1:** Utilizando SQL *(0.25 puntos)*\n",
    "            - **Parte 2.2.1.2:** Utilizando RDD *(1 punto)*\n",
    "        - **Parte 2.2.2:** Contar hashtags *(1.5 puntos)*\n",
    "- **Parte 3:** Sampling *(3 Puntos)*\n",
    "    - **Parte 3.1:** Homogéneo *(1 punto)*\n",
    "    - **Parte 3.2:** Estratificado *(1.5 puntos)*\n",
    "- **Parte 4**: Introducción a los datos relacionales *(2 puntos)*\n",
    "    - **Parte 4.1:** Generar la red de retweets *(1 punto)*\n",
    "        - **Parte 4.1.1**: Construcción de la edgelist *(0.5 puntos)*\n",
    "        - **Parte 4.1.2**: Centralidad de grado *(0.5 puntos)*\n",
    "    - **Parte 4.2:** Análisis de redes utilitzando GraphFrames *(1 punto)*\n",
    "        - **Parte 4.2.1:** Crear un graph frame (0.5 puntos)\n",
    "        - **Parte 4.2.2:** Centralidad PageRank (0.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 0:** Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages graphframes:graphframes:0.7.0-spark2.4-s_2.11 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "# Introducid el nombre de la app PEC3_ seguido de vuestro nombre de usuario\n",
    "conf.setAppName('PEC3_antoniogmartin')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 1:** Introducción a data frames estructurados i operaciones sobre ellos.\n",
    "\n",
    "Como ya se ha mencionado, en esta práctica vamos ha utilizar datos de Twitter que recolectamos durante las elecciones generales en España del 28 de abril de 2019. Como veremos, los tweets tienen una estructura interna bastante compleja que hemos simplificado un poco en esta práctica.\n",
    "\n",
    "### **Parte 1.1:** Importar los datos\n",
    "\n",
    "Lo primero que vamos ha aprender es cómo importar este tipo de datos a nuestro entorno. Uno de los tipos de archivos más comunes para guardar este formato de información es [la estructura JSON](https://en.wikipedia.org/wiki/JSON). Esta estructura permite guardar información en un texto plano de diferentes objetos siguiendo una estructura de diccionario donde cada campo tiene asignado una llave y un valor. La estructura puede ser anidada, o sea que una llave puede tener como valor otra estructura tipo diccionario.\n",
    "\n",
    "Spark SQL permite leer datos de muchos formatos diferentes (como recordareis de la anterior práctica donde leímos un fichero CSV). En esta ocasión, se os pide que leáis un fichero JSON de la ruta ```/aula_M2.858/data/tweets28a_sample.json```. Este archivo contiene un pequeño *sample*, un 0.1% de la base de datos completa (en un siguiente apartado veremos cómo realizar este *sampleado*). En esta ocasión no se os pide especificar la estructura del data frame ya que la función de lectura la inferirá automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 27268 tweets\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "tweets_sample = sqlContext.read.json(\"/aula_M2.858/data/tweets28a_sample.json\")\n",
    "\n",
    "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es mostrar la estructura del dataset que acabamos de cargar. Recordad que podéis obtener la información acerca de cómo está estructurado el DataTable utilizando el método ```printSchema()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis observar que la estructura del tweet contiene múltiples campos anidados. Teneis que familiarizaros con esta estructura ya que será la que utilizaremos durante toda la práctica. Recordad también que no todos los tweets tienen todos los campos, como por ejemplo la ubicación (campo ```place```). Cuando esto pasa el campo pasa a ser ```NULL```. Podéis ver mas información sobre este tipo de datos en [este enlace](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 1.2:** *Queries* sobre sobre data frames complejos\n",
    "\n",
    "En la anterior práctica hemos visto cómo hacer consultas sobre un dataset muy simple utilizando sentencias *SQL*. En esta parte vamos a refrescar los conceptos utilizados en la práctica anterior introduciendo algunos conceptos más avanzados y una nueva manera de trabajar sobre data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 1.2.1:** Queries SQL\n",
    "\n",
    "Como recordaréis de la parte 3 de la anterior PEC, el primer paso consiste en registrar la tabla en el contexto SQL comprobando primero si existe y borrándola en el caso que sea así. En este apartado se os pide que registréis la tabla ```tweets_sample``` que acabamos de cargar en el contexto sql bajo el mismo nombre ```tweets_sample```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_sample\")\n",
    "sqlContext.registerDataFrameAsTable(tweets_sample,\"tweets_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se os pide que creeis una tabla ```users_agg``` con [la información agregada](https://www.w3schools.com/sql/sql_groupby.asp) de los usuarios que tengan definido su idioma (```user.lang```) como español (```es```). En concreto se os pide que la tabla contenga las siguientes columnas:\n",
    "- **screen_name:** nombre del usuario\n",
    "- **friends_count:** número máximo (ver nota) de personas a las que sigue\n",
    "- **tweets:** número de tweets realizados\n",
    "- **followers_count:** número máximo (ver nota) personas que siguen al usuario.\n",
    "\n",
    "El orden en el cual se deben mostrar los registros es orden descendente acorde al número de tweets.\n",
    "\n",
    "***Nota:*** es importante que os fijéis que el nombre de *friends* i *followers* puede diferir a lo largo de la adquisición de datos. En este caso vamos ha utilizar la función de agregación ```MAX``` sobre cada uno de estos campos para evitar segmentar el usuario en diversas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users = sqlContext.sql(\"SELECT user.id_str, retweeted_status,text from tweets_sample where user.screen_name='anaoromi'\")\n",
    "#users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+-------------+---------------+----+\n",
      "|    screen_name|tweets|friends_count|followers_count|lang|\n",
      "+---------------+------+-------------+---------------+----+\n",
      "|       anaoromi|    16|         6258|           6774|  es|\n",
      "|    RosaMar6254|    14|         6208|           6245|  es|\n",
      "|        lyuva26|    13|         3088|           3732|  es|\n",
      "|PisandoFuerte10|    12|         2795|           1752|  es|\n",
      "|     carrasquem|    12|          147|            215|  es|\n",
      "|       jasalo54|    11|         1889|            689|  es|\n",
      "|      lolalailo|     9|         4922|           3738|  es|\n",
      "|  Rafa_eltorete|     9|          908|           1060|  es|\n",
      "| locuspolitikus|     9|        11261|          10244|  es|\n",
      "|  PabloChabolas|     9|         4925|           4042|  es|\n",
      "+---------------+------+-------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg = sqlContext.sql(\"SELECT user.screen_name,COUNT(user.screen_name) as tweets,MAX(user.friends_count) as friends_count,MAX(user.followers_count) as followers_count,user.lang as lang FROM tweets_sample where user.lang='es' GROUP BY user.lang, user.screen_name ORDER BY COUNT(user.screen_name) DESC \")\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = users_agg.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginad ahora que queremos combinar la información que acabamos de generar con información acerca del número de veces que un usuario ha sido retuiteado. Para hacer este tipo de combinaciones necesitamos recurrir al [```JOIN``` de tablas](https://www.w3schools.com/sql/sql_join.asp). Primero debemos registrar la tabla que acabamos de generar en el contexto SQL. Recordad que primero debéis comprobar si la tabla existe y en caso afirmativo eliminarla. La tabla tenéis que registrarla bajo el nombre de ```user_agg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS user_agg\")\n",
    "sqlContext.registerDataFrameAsTable(users_agg,\"user_agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez registrada se pide que combinéis esta tabla y la tabla ```tweets_sample``` utilizando un ```INNER JOIN``` para obtener una nueva tabla con la siguiente información:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***friends_count:*** número máximo de personas a las que sigue\n",
    "- ***followers_count:*** número máximo de personas que siguen al usuario.\n",
    "- ***tweets:*** número de tweets realizados por el usuario.\n",
    "- ***retweeted:*** número de retweets obtenidos por el usuario.\n",
    "- ***ratio_tweet_retweeted:*** ratio de retweets por número de tweets publicados $\\frac{retweets}{tweets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---------+-------------+---------------+---------------------+\n",
      "|   screen_name|tweets|retweeted|friends_count|followers_count|ratio_tweet_retweeted|\n",
      "+--------------+------+---------+-------------+---------------+---------------------+\n",
      "|          PSOE|     1|      155|        13635|         671073|                155.0|\n",
      "|  CiudadanosCs|     1|      117|        92910|         511896|                117.0|\n",
      "|     JuntsXCat|     1|       73|          202|          88515|                 73.0|\n",
      "|  PartidoPACMA|     1|       63|         1498|         232932|                 63.0|\n",
      "|  pablocasado_|     1|       50|         4567|         238926|                 50.0|\n",
      "|voxnoticias_es|     1|       44|         2146|          29582|                 44.0|\n",
      "|RaiLopezCalvet|     1|       43|         7579|          13574|                 43.0|\n",
      "|        iunida|     1|       39|        10225|         558318|                 39.0|\n",
      "|        Xuxipc|     1|       37|          311|         184967|                 37.0|\n",
      "|       Panik81|     1|       29|         1587|          15374|                 29.0|\n",
      "+--------------+------+---------+-------------+---------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = sqlContext.sql(\"\"\"\n",
    "select B.rt_name as screen_name,MAX(A.tweets) as tweets,MAX(B.retweeted) as retweeted,\n",
    "MAX(A.friends_count) as friends_count,\n",
    "MAX(A.followers_count) as followers_count,\n",
    "MAX(B.retweeted/A.tweets) as ratio_tweet_retweeted \n",
    "\n",
    "FROM user_agg A INNER JOIN\n",
    "(select retweeted_status.user.screen_name as rt_name,\n",
    "count(retweeted_status.user.screen_name) as retweeted\n",
    "from tweets_sample group by retweeted_status.user.screen_name) B \n",
    "ON A.screen_name=B.rt_name GROUP BY B.rt_name ORDER BY retweeted DESC\n",
    "\"\"\")\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 1.2.2:** Queries a través del pipeline\n",
    "\n",
    "Las tablas de Spark SQL ofrecen otro mecanismo para aplicar las transformaciones y obtener resultados similares a los que se obtendría aplicando una consulta SQL. Por ejemplo utilizando el siguiente pipeline obtendremos el texto de todos los tweets en español:\n",
    "\n",
    "```\n",
    "tweets_sample.where(\"lang == 'es'\").select(\"text\")\n",
    "```\n",
    "\n",
    "Que es equivalente a la siguiente sentencia SQL:\n",
    "\n",
    "```\n",
    "SELECT text\n",
    "FROM tweets_sample\n",
    "WHERE lang == 'es'\n",
    "```\n",
    "\n",
    "Podéis consultar el [API de spark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) para encontrar más información sobre como utilitzar las diferentes transformaciones en tablas.\n",
    "\n",
    "En este ejercicio se os pide que repliquéis la query obtenida en el apartado anterior empezando por generar la tabla ```users_agg```. Podéis utilizar las transformaciones ```where```, ```select``` (o ```selectExpr```), ```groupBy```, ```count```, ```agg``` y ```orderBy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------------------+------------------+--------------------+\n",
      "|    screen_name|lang|max(friends_count)|count(screen_name)|max(followers_count)|\n",
      "+---------------+----+------------------+------------------+--------------------+\n",
      "|       anaoromi|  es|              6258|                16|                6774|\n",
      "|    RosaMar6254|  es|              6208|                14|                6245|\n",
      "|        lyuva26|  es|              3088|                13|                3732|\n",
      "|     carrasquem|  es|               147|                12|                 215|\n",
      "|PisandoFuerte10|  es|              2795|                12|                1752|\n",
      "|       jasalo54|  es|              1889|                11|                 689|\n",
      "| locuspolitikus|  es|             11261|                 9|               10244|\n",
      "|      kikyosanz|  es|               154|                 9|                 273|\n",
      "|  Rafa_eltorete|  es|               908|                 9|                1060|\n",
      "|      lolalailo|  es|              4922|                 9|                3738|\n",
      "+---------------+----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#users_agg = sqlContext.sql(\"SELECT user.screen_name,COUNT(user.screen_name) as tweets,MAX(user.friends_count) as friends_count,MAX(user.followers_count) as followers_count,user.lang as lang FROM tweets_sample where user.lang='es' GROUP BY user.lang, user.screen_name ORDER BY COUNT(user.screen_name) DESC \")\n",
    "\n",
    "users = tweets_sample.where(\"user.lang=='es'\").select(\"user.id_str\",\"user.screen_name\",\"user.lang\",\"user.friends_count\",\"user.followers_count\")\n",
    "\n",
    "users_agg = users.groupBy(\"screen_name\",\"lang\")\\\n",
    "                 .agg({\"friends_count\":\"max\",\"followers_count\":\"max\",\"screen_name\":\"count\"})\\\n",
    "                 .orderBy(\"count(screen_name)\",ascending=False)\n",
    "\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si os fijáis veréis que el nombre de las columnas no corresponde con el obtenido anteriormente, podéis cambiar el nombre de una columna determinada utilizando la transformación ```withColumnRenamed```. Cambiad el nombre de las columnas para que coincidan con el apartado anterior y guardadlas en una variable ```user_agg_new```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-------------+------+---------------+\n",
      "|    screen_name|lang|friends_count|tweets|followers_count|\n",
      "+---------------+----+-------------+------+---------------+\n",
      "|       anaoromi|  es|         6258|    16|           6774|\n",
      "|    RosaMar6254|  es|         6208|    14|           6245|\n",
      "|        lyuva26|  es|         3088|    13|           3732|\n",
      "|     carrasquem|  es|          147|    12|            215|\n",
      "|PisandoFuerte10|  es|         2795|    12|           1752|\n",
      "|       jasalo54|  es|         1889|    11|            689|\n",
      "| locuspolitikus|  es|        11261|     9|          10244|\n",
      "|      kikyosanz|  es|          154|     9|            273|\n",
      "|  Rafa_eltorete|  es|          908|     9|           1060|\n",
      "|      lolalailo|  es|         4922|     9|           3738|\n",
      "+---------------+----+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg_new = users_agg.withColumnRenamed('max(friends_count)','friends_count')\\\n",
    "                         .withColumnRenamed('count(screen_name)','tweets')\\\n",
    "                         .withColumnRenamed('max(followers_count)','followers_count')\n",
    "\n",
    "users_agg_new.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = users_agg_new.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cread ahora una tabla ```user_retweets``` utilizando transformaciones que contenga dos columnas:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***retweeted:*** número de retweets\n",
    "\n",
    "Podéis utilizar las mismas transformaciones que en el ejercicio anterior. Ordenad la tabla en orden descendente utilizando el valor de la columna ```retweeted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-------------+\n",
      "|   screen_name|count(screen_name)|count(id_str)|\n",
      "+--------------+------------------+-------------+\n",
      "|        vox_es|               299|          299|\n",
      "|  ahorapodemos|               238|          238|\n",
      "| Santi_ABASCAL|               238|          238|\n",
      "|      iescolar|               166|          166|\n",
      "| AlbanoDante76|               161|          161|\n",
      "|          PSOE|               155|          155|\n",
      "|AntonioMaestre|               154|          154|\n",
      "|          KRLS|               149|          149|\n",
      "|        boye_g|               142|          142|\n",
      "|  CiudadanosCs|               117|          117|\n",
      "+--------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_retweets_aux = tweets_sample.select(\"retweeted_status.user.screen_name\",\"retweeted_status.user.id_str\")\n",
    "\n",
    "user_retweets = user_retweets_aux.groupBy(\"screen_name\")\\\n",
    "                                 .agg({\"id_str\": \"count\",\"screen_name\": \"count\"})\\\n",
    "                                 .orderBy(\"count(screen_name)\", ascending=False)\n",
    "user_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_retweets=user_retweets.withColumnRenamed('max(friends_count)','friends_count')\\\n",
    "                         .withColumnRenamed('count(screen_name)','retweeted')\\\n",
    "                         .withColumnRenamed('count(id_str)','retweets_by_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = user_retweets.first()\n",
    "assert output.screen_name == 'vox_es' and output.retweeted == 299, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de combinar dos tablas es utilizando el [metodo de tabla ```join```](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). Combinad la información de la tabla ```users_agg_new``` y ```user_retweets``` en una nueva tabla ```retweeted``` utilizando la columna ```screen_name```. Ordenad la nueva tabla en orden descendente con el nombre de retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = users_agg_new.alias('df1')\n",
    "df2 = user_retweets.alias('df2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+\n",
      "|   screen_name|lang|friends_count|tweets|followers_count|   screen_name|retweeted|retweets_by_id|\n",
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+\n",
      "|          PSOE|  es|        13635|     1|         671073|          PSOE|      155|           155|\n",
      "|  CiudadanosCs|  es|        92910|     1|         511896|  CiudadanosCs|      117|           117|\n",
      "|     JuntsXCat|  es|          202|     1|          88515|     JuntsXCat|       73|            73|\n",
      "|  PartidoPACMA|  es|         1498|     1|         232932|  PartidoPACMA|       63|            63|\n",
      "|  pablocasado_|  es|         4567|     1|         238926|  pablocasado_|       50|            50|\n",
      "|voxnoticias_es|  es|         2146|     1|          29582|voxnoticias_es|       44|            44|\n",
      "|RaiLopezCalvet|  es|         7579|     1|          13574|RaiLopezCalvet|       43|            43|\n",
      "|        iunida|  es|        10225|     1|         558318|        iunida|       39|            39|\n",
      "|        Xuxipc|  es|          311|     1|         184967|        Xuxipc|       37|            37|\n",
      "|       Panik81|  es|         1587|     1|          15374|       Panik81|       29|            29|\n",
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retweeted = df1.join(df2,df1.screen_name == df2.screen_name)\\\n",
    "                         .orderBy(df2.retweeted, ascending=False)\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notaréis que algunos de los registros que aparecen en la tabla ```users_retweeted``` no están presentes en la tabla retweeted. Esto es debido a que, por defecto, el método aplica un inner join y por tanto solo combina los registros presentes en ambas tablas. Podéis cambiar este comportamiento a través de los parámetros de la función.\n",
    "\n",
    "Para terminar esta parte y reconstruir el resultado del apartado 1.2.1 vamos a añadir una columna ```ratio_tweet_retweeted``` con información del ratio entre retweets y tweets. Para ello debéis utilizar la transformación ```withColumn```. El resultado debe estar ordenado considerando esta nueva columna en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+---------------------+\n",
      "|   screen_name|lang|friends_count|tweets|followers_count|   screen_name|retweeted|retweets_by_id|ratio_tweet_retweeted|\n",
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+---------------------+\n",
      "|          PSOE|  es|        13635|     1|         671073|          PSOE|      155|           155|                155.0|\n",
      "|  CiudadanosCs|  es|        92910|     1|         511896|  CiudadanosCs|      117|           117|                117.0|\n",
      "|     JuntsXCat|  es|          202|     1|          88515|     JuntsXCat|       73|            73|                 73.0|\n",
      "|  PartidoPACMA|  es|         1498|     1|         232932|  PartidoPACMA|       63|            63|                 63.0|\n",
      "|  pablocasado_|  es|         4567|     1|         238926|  pablocasado_|       50|            50|                 50.0|\n",
      "|voxnoticias_es|  es|         2146|     1|          29582|voxnoticias_es|       44|            44|                 44.0|\n",
      "|RaiLopezCalvet|  es|         7579|     1|          13574|RaiLopezCalvet|       43|            43|                 43.0|\n",
      "|        iunida|  es|        10225|     1|         558318|        iunida|       39|            39|                 39.0|\n",
      "|        Xuxipc|  es|          311|     1|         184967|        Xuxipc|       37|            37|                 37.0|\n",
      "|       Panik81|  es|         1587|     1|          15374|       Panik81|       29|            29|                 29.0|\n",
      "+--------------+----+-------------+------+---------------+--------------+---------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = retweeted.withColumn(\"ratio_tweet_retweeted\",retweeted.retweeted/retweeted.tweets)\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 2:** Bases de datos HIVE y operaciones complejas\n",
    "\n",
    "Hasta ahora hemos estado trabajando con un pequeño sample de los tweets generados (el 0.1%). En esta parte de la PEC vamos a ver como trabajar y tratar con el dataset completo. Para ello vamos ha utilizar tanto transformaciones sobre tablas como operaciones sobre RDD cuando sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2.1:** Bases de datos Hive\n",
    "\n",
    "Muchas veces los datos con los que vamos ha trabajar se van a utilizar en diversos proyectos. Una manera de organizar los datos es, en lugar de utilizar directamente los ficheros, recurrir a una base de datos para gestionar la información. En el entorno Hadoop una de las bases de datos más utilizadas es [Apache Hive](https://hive.apache.org/), una base de datos que permite trabajar con contenido distribuido.\n",
    "\n",
    "La manera de acceder a esta base de datos es creando un contexto Hive de manera muy similar a como declaramos un contexto SQL. Primero de todo vamos a declarar un variable ```hiveContext``` instanciándola como un objeto de la classe ```HiveContext```. Acto seguido vamos a comprobar cuantas tablas están registradas en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+\n",
      "|database|       tableName|isTemporary|\n",
      "+--------+----------------+-----------+\n",
      "| default|    province_28a|      false|\n",
      "| default|       tweets28a|      false|\n",
      "| default|       user_info|      false|\n",
      "|        |   tweets_sample|       true|\n",
      "|        |tweets_sampling3|       true|\n",
      "|        |        user_agg|       true|\n",
      "+--------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext = HiveContext(sc)\n",
    "hiveContext.tables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observad que ahora mismo tenemos cinco tablas registradas en este contexto. Tres de ellas no temporales y dos temporales, las que hemos registrado previamente. Por tanto sqlContext y hiveContext están concetados (es la misma sessión)\n",
    "\n",
    "Vamos ha crear una variable ```tweets``` que utilizaremos para acceder a la tabla ```tweets28a``` guardada en ```hiveContext``` utilizando para ello el método ```table()``` de este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 25419835 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets =  hiveContext.table(\"tweets28a\")\n",
    "print(\"Loaded dataset contains {} tweets\".format(tweets.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el mismo método que en el apartado 1.1, comprobad la estructura de la tabla que acabamos de cargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 2.2:** Más allá de las transformaciones SQL\n",
    "\n",
    "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van más allá de lo que podemos conseguir utilizando el lenguaje SQL. En esta parte de la práctica vamos practicar cómo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### **Parte 2.2.1:** Tweets por población\n",
    "##### **Parte 2.2.1.1:** Utilizando SQL\n",
    "Un pequeño porcentaje, alrededor del 1%, de los tweets realizados está geolocalizado. Eso quiere decir que para estos tweets tenemos información acerca del lugar donde han sido realizados guardado en el campo ```place```. En este ejercicio se pide que utilizando una sentencia SQL mostréis en orden descendente cuántos tweets se han realizado en cada lugar. La tabla resultante ```tweets_place``` debe tener las siguientes columnas:\n",
    "- ***name:*** nombre del lugar\n",
    "- ***tweets:*** número de tweets\n",
    "\n",
    "Recordad que no todos los tweets en la base de datos tienen que tener información geolocalizada, tenéis que filtrarlos teniendo en cuenta todos los que tienen un valor no nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|Villamartín|  2364|\n",
      "|     Málaga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = hiveContext.sql(\"\"\"\n",
    "select place.name,count(user.id_str) as tweets\n",
    "from tweets28a WHERE place is not NULL \n",
    "GROUP BY place.name ORDER BY count(user.id_str) DESC\n",
    "\"\"\")\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parte 2.2.1.2:** Utilizando RDD\n",
    "\n",
    "Ahora se os pide que hagáis lo mismo pero esta vez utilizando RDD para realizar la agregación (recordad los ejercicios de contar palabras que hicisteis en la PEC 1).\n",
    "\n",
    "El primer paso consiste en generar un tabla ```tweets_geo``` que solo contenga información de tweets geolocalizados con una sola columna:\n",
    "- ***name:*** nombre del lugar desde donde se ha generado el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbutils.fs.rm(\"dbfs:/user/hive/warehouse/tweets_geo/\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiveContext.sql(\"\"\"\n",
    " DROP TABLE IF EXISTS tweets_geo3\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_geo = hiveContext.sql(\"\"\"\n",
    "CREATE TABLE tweets_geo3 AS \n",
    "(SELECT place.name from tweets28a WHERE\n",
    "place is not NULL)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|Las Palmas de Gra...|\n",
      "|Las Palmas de Gra...|\n",
      "|    Collado Villalba|\n",
      "|            Palencia|\n",
      "|               Egüés|\n",
      "|             Córdoba|\n",
      "|Castellón de la P...|\n",
      "| Granadilla de Abona|\n",
      "|San Vicente del R...|\n",
      "|              Madrid|\n",
      "|         El Vendrell|\n",
      "|             Granada|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_geo=hiveContext.sql(\"\"\"\n",
    "SELECT * from tweets_geo3\n",
    "\"\"\")\n",
    "tweets_geo.limit(12).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora viene la parte interesante. Una tabla puede convertirse en un RDD a través del atributo ```.rdd```. Este atributo guarda la información de la tabla en una lista donde cada elemento es un [objeto del tipo ```Row```](https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html). Los objetos pertenecientes a esta clase pueden verse como diccionarios donde la información de las diferentes columnas queda reflejada en forma de atributo. Por ejemplo, imaginad que tenemos una tabla con dos columnas, nombre y apellido, si utilizamos el atributo ```.rdd``` de dicha tabla obtendremos una lista con objetos del tipo row donde cada objeto tiene dos atributos: nombre y apellido. Para acceder a los atributos solo tenéis que utilizar la sintaxis *punto* de Python, e.g., ```row.nombre``` o ```row.apellido```.\n",
    "\n",
    "En esta parte del ejercicio se os pide que creeis un objeto ```tweets_lang_rdd``` que contenga una lista de tuplas con la información ```(name, tweets)``` sobre el nombre del lugar y el número de tweets generados desde allí. Recordad el ejercicio de contar palabras de la PEC 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_place_rdd = tweets_geo.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='Las Palmas de Gran Canaria')]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_place_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "tweets_place_rdd=tweets_place_rdd.map(lambda a: (a[0], 1))\n",
    "tweets_place_rdd=tweets_place_rdd.reduceByKey(add).sortBy(lambda a: a[1],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Madrid', 19655), ('Barcelona', 13987), ('Sevilla', 3820), ('Valencia', 2833), ('Zaragoza', 2449), ('Villamartín', 2364), ('Málaga', 2184), ('Murcia', 1800), ('Granada', 1637), ('Alicante', 1628), ('Palma', 1597), ('Gijón', 1421), ('Oviedo', 1318), ('Las Palmas de Gran Canaria', 1290), ('Valladolid', 1272), ('Vigo', 1235), ('Córdoba', 1048), ('Girona', 957), ('Terrassa', 951), (\"L'Hospitalet de Llobregat\", 909)]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_place_rdd.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez generado este RDD vamos a crear un tabla. El primer paso es generar por cada tupla un objeto Row que contenga un atributo ```name``` y un atributo ```tweets```. Ahora solo tenéis que aplicar el método ```toDF()``` para generar una tabla. Ordenad las filas de esta tabla por el número de tweets en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|Villamartín|  2364|\n",
      "|     Málaga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = tweets_place_rdd.toDF().withColumnRenamed('_1','name').withColumnRenamed('_2','tweets')\n",
    "\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 2.2.2:** Contar hashtags\n",
    "\n",
    "En el ejercicio anterior hemos visto cómo podemos generar la misma información haciendo una agregación mediante SQL o utilizando RDDs. Como seguro que habéis observado la semántica de la sentencia SQL es mucho más limpia para realizar esta tarea. Pero no todas las tareas que os vais a encontrar se pueden hacer mediante sentencias SQL. En este ejercicio vamos a ver un ejemplo.\n",
    "\n",
    "El objetivo de este ejercicio es contar el número de veces que cada hashtag (palabras precedidas por un #) ha aparecido en el dataset. Para evitar la sobrerrepresentación debida a los retweets vamos a concentrarnos en solo aquellos tweets que no son retweets de ningún otro, o dicho de otra manera, en aquellos en los que el campo ```retweeted_status``` es nulo. Cread una variable ```non_retweets``` que contenga todos estos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|@Pablo_Iglesias_ ...|\n",
      "|@josebouvila @Ada...|\n",
      "|— Mariano Rajoy ¿...|\n",
      "|Vamos a ver... SI...|\n",
      "|@albertoertoo Por...|\n",
      "|@FrancescFalip @A...|\n",
      "|Hey Vox, we just ...|\n",
      "|         sub: normal|\n",
      "|@JorgeBustos1 Tra...|\n",
      "|@sanchezdelreal @...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_retweets = hiveContext.sql(\"\"\"\n",
    "SELECT text from tweets28a WHERE retweeted_status IS NULL\n",
    "\"\"\")\n",
    "non_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente vamos ha crear una variable ```hashtags``` que contenga una lista de tuplas con la información ```(hashtag, count)```. Para ello, cread un RDD que contenga una lista con el texto de todos los tweets. Una vez hecho este paso tenéis que extraer los hashtags (palabras precedidas por un #) y contarlos.\n",
    "\n",
    "Recordad los conocimientos adquiridos en la PEC 1 y el anterior ejercicio, os serán de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(text='@Pablo_Iglesias_ @MiguelTheFaker Pablo ya verás como se entere más de uno que había sitio en bodega y has ido en turista')]\n"
     ]
    }
   ],
   "source": [
    "non_retweets_rdd = non_retweets.rdd\n",
    "print(non_retweets_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001#002#003rjriwd#004#005##006###007\n",
      "álvarez\n",
      "then#spaces#s\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def keepHashTag(text):\n",
    "    minusc = text.lower()\n",
    "    hashtag= re.sub(r'([^\\#\\w+])+\\w*','', minusc)\n",
    "    hashtag=re.sub(r'^(\\w+)','', hashtag)\n",
    "    res = hashtag.strip('#')\n",
    "    return res\n",
    "    \n",
    "print(keepHashTag(\"\"\"#001\n",
    "dlfkslaklfs #002 #003rjriwd#004\n",
    "#005##006###007\"\"\"))\n",
    "print(keepHashTag('Cayetana #Álvarez es de Pueblo Paleta.'))\n",
    "print(keepHashTag(' *      Remove punctuation #then#spaces#s  * '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtagCount(hashtag):\n",
    "    return hashtag.map(lambda a: (a, 1)).reduceByKey(add).sortBy(lambda kv: (kv[1]),ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = non_retweets_rdd.map(lambda a: keepHashTag(a[0])).filter(lambda x: len(x)>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['valorseguro',\n",
       " 'yovotovox#abascalpresidente#hazquearrasevox#voxavanza#voxextremanecesidad#voxsaleaganar',\n",
       " 'vox',\n",
       " 'l6nencampaña#28a#votapsoe',\n",
       " 'l6nnotredame#juevespsanto#debatertve#rumbournasarv#cuatroaldia44#elintermedio#españaviva#ar18a',\n",
       " '28a']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_splitted =  hashtags.flatMap(lambda x: x.split(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['valorseguro',\n",
       " 'yovotovox',\n",
       " 'abascalpresidente',\n",
       " 'hazquearrasevox',\n",
       " 'voxavanza',\n",
       " 'voxextremanecesidad',\n",
       " 'voxsaleaganar',\n",
       " 'vox',\n",
       " 'l6nencampaña',\n",
       " '28a',\n",
       " 'votapsoe',\n",
       " 'l6nnotredame',\n",
       " 'juevespsanto',\n",
       " 'debatertve',\n",
       " 'rumbournasarv',\n",
       " 'cuatroaldia44',\n",
       " 'elintermedio',\n",
       " 'españaviva',\n",
       " 'ar18a',\n",
       " '28a']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags_splitted.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_result = hashtagCount(hashtags_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se os pide que con el RDD obtenido generéis una tabla ```hashtagsTable``` compuesta de dos columnas:\n",
    "- ***hashtag***\n",
    "- ***num:*** número de veces que aparece cada hashtag.\n",
    "\n",
    "Ordenadla en orden descendente por número de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             hashtag|   num|\n",
      "+--------------------+------+\n",
      "|                 28a|172083|\n",
      "|    eldebatedecisivo|114066|\n",
      "|      eldebateenrtve|104852|\n",
      "|eleccionesgeneral...| 35140|\n",
      "|      equiparacionya| 33777|\n",
      "|        eleccionesl6| 31187|\n",
      "|          hazquepase| 29011|\n",
      "|    debateatresmedia| 23286|\n",
      "|                 vox| 22280|\n",
      "|          debatertve| 18991|\n",
      "|lahistorialaescri...| 18097|\n",
      "|            debattv3| 18027|\n",
      "|           porespaña| 17054|\n",
      "|            votapsoe| 16733|\n",
      "| eleccionesgenerales| 15905|\n",
      "|          ilpjusapol| 15768|\n",
      "|             28abril| 15089|\n",
      "|          españaviva| 13163|\n",
      "|         valorseguro| 12224|\n",
      "|              españa| 12204|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtagsTable =  hashtags_result.toDF().withColumnRenamed('_1','hashtag').withColumnRenamed('_2','num')\n",
    "hashtagsTable.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172083\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-85650fe8d4e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashtagsTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#28A\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m158124\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output"
     ]
    }
   ],
   "source": [
    "# Compruebese mi expresión regular que funciona bastante bien\n",
    "output = hashtagsTable.first()\n",
    "print(output.num)\n",
    "assert output.hashtag == \"#28A\" and output.num == 158124, \"Incorrect output\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 3:** Sampling\n",
    "\n",
    "En muchas ocasiones, antes de lanzar costoso procesos, es una práctica habitual tratar con un pequeño conjunto de los datos para investigar algunas propiedades o simplemente para debugar nuestros algoritmos, a esta tarea se la llama sampling. En esta parte de la práctica vamos a ver los dos principales métodos de sampling y cómo utilizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3.1:** Homogeneo\n",
    "\n",
    "El primer sampling que vamos a ver es [el homogeneo](https://en.wikipedia.org/wiki/Simple_random_sample). Este sampling se basta en simplemente escoger una fracción de la población seleccionando aleatoriamente elementos de la misma.\n",
    "\n",
    "Primero de todo vamos ha realizar un sampling homogéneo del 1% de los tweets generados en periodo electoral sin reemplazo. Guardad en una variable ```tweets_sample``` este sampling utilizando el método ```sample``` descrito en la [API de pyspark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). El seed que vais a utilizar para inicializar el generador aleatorio es 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets sampled: 254185\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "tweets_sample = tweets.sample(fraction, seed)\n",
    "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tweets_sample.count() == 254185, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las cosas que resulta interesante comprobar acerca de los patrones de uso de las redes sociales es el patrón de uso diario. En este caso nos interesa el número promedio de tweets que se genera cada hora del día. Para extraer esta información lo que haremos primero, será generar una tabla ```tweets_timestamp``` con la información:\n",
    "- ***created_at***: timestamp de cuando se publicó el tweet.\n",
    "- ***hour***: a que hora del dia corresponde.\n",
    "- ***day***: Fecha en formato MM-dd-YY\n",
    "\n",
    "La fecha que figura en la base de datos esta en la franja horaria GMT. El primer paso es pasar esta información al horario peninsular de España, podéis utilizar la función ```from_utc_timestamp``` para este fin. Así mismo, la función ```hour``` os servirá para extraer la hora del timestamp y la función ```date_format``` os permitirá generar la fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_sampling3\")\n",
    "sqlContext.registerDataFrameAsTable(tweets_sample,\"tweets_sampling3\")\n",
    "\n",
    "#tweets_sample.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------+\n",
      "|         created_at|hour|     day|\n",
      "+-------------------+----+--------+\n",
      "|2019-04-21 04:24:26|   4|04-21-19|\n",
      "|2019-04-21 04:24:44|   4|04-21-19|\n",
      "|2019-04-21 04:24:46|   4|04-21-19|\n",
      "|2019-04-21 04:25:50|   4|04-21-19|\n",
      "|2019-04-21 04:25:53|   4|04-21-19|\n",
      "|2019-04-21 04:25:59|   4|04-21-19|\n",
      "|2019-04-21 04:26:21|   4|04-21-19|\n",
      "|2019-04-21 04:27:31|   4|04-21-19|\n",
      "|2019-04-21 04:28:01|   4|04-21-19|\n",
      "|2019-04-21 04:28:09|   4|04-21-19|\n",
      "|2019-04-21 04:28:14|   4|04-21-19|\n",
      "|2019-04-21 04:28:21|   4|04-21-19|\n",
      "|2019-04-21 04:28:35|   4|04-21-19|\n",
      "|2019-04-21 04:28:53|   4|04-21-19|\n",
      "|2019-04-21 04:29:10|   4|04-21-19|\n",
      "|2019-04-21 04:29:34|   4|04-21-19|\n",
      "|2019-04-21 04:29:39|   4|04-21-19|\n",
      "|2019-04-21 04:29:56|   4|04-21-19|\n",
      "|2019-04-21 04:30:06|   4|04-21-19|\n",
      "|2019-04-21 04:30:25|   4|04-21-19|\n",
      "+-------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=tweets_sample.withColumn(\"created_at\",f.from_utc_timestamp(tweets_sample.created_at, \"CET\").alias('tstamp'))\n",
    "tweets_timestamp=df.select(df.created_at,\\\n",
    "                     f.hour(df.created_at).alias(\"hour\"),\\\n",
    "                    f.date_format(df.created_at,'MM-dd-YY').alias(\"day\"))\n",
    "tweets_timestamp.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es agregar estos datos por hora y día en una tabla ```tweets_hour_day```. Tenéis que crear una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** hora del dia\n",
    "- ***day:*** fecha\n",
    "- ***count:*** número de tweets generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+--------+-----+\n",
      "|    screen_name|hour|     day|count|\n",
      "+---------------+----+--------+-----+\n",
      "|Antonia85728101|   7|04-23-19|    7|\n",
      "|      SantiRR84|  23|04-22-19|    7|\n",
      "|       Maizonin|  22|04-23-19|    7|\n",
      "|  Jaime76610897|   0|04-24-19|    6|\n",
      "|         fcr501|   1|04-23-19|    6|\n",
      "|AlmodovarMedina|   1|04-24-19|    6|\n",
      "|        vraleon|   0|04-23-19|    6|\n",
      "| Carloschuchero|   1|04-23-19|    6|\n",
      "| forxas_pequeno|  23|04-26-19|    6|\n",
      "|        tortu80|   0|04-24-19|    6|\n",
      "+---------------+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=tweets_sample.withColumn(\"created_at\",f.from_utc_timestamp(tweets_sample.created_at, \"CET\").alias('created_at'))\n",
    "tweets_hour_day_aux=df.select(\"user.screen_name\",\\\n",
    "                     f.hour(df.created_at),\\\n",
    "                    f.date_format(df.created_at,'MM-dd-YY'))\n",
    "tweets_hour_day_aux=tweets_hour_day_aux.withColumnRenamed(\"hour(created_at)\",\"hour\").withColumnRenamed(\"date_format(created_at, MM-dd-YY)\",\"day\")\n",
    "tweets_hour_day=tweets_hour_day_aux.groupBy(\"screen_name\",\"hour\",\"day\").agg({\"screen_name\":\"count\"}).orderBy(\"count(screen_name)\", ascending=False)\n",
    "tweets_hour_day=tweets_hour_day.withColumnRenamed(\"count(screen_name)\",\"count\")\n",
    "tweets_hour_day.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último solo nos queda hacer una agregación por hora para conseguir el promedio de tweets por hora. Tenéis que generar una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** Hora\n",
    "- ***tweets:*** Promedio de tweets realizados\n",
    "\n",
    "Recordad que estamos trabajando con un sample del 1% por tanto tenéis que corregir la columna ```tweets``` para que refleje el promedio que deberíamos esperar en el conjunto completo de tweets. La tabla tiene que estar ordenada en orden ascendente de hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----+\n",
      "|    screen_name|hour|count|\n",
      "+---------------+----+-----+\n",
      "|   scarabelo150|  22|   16|\n",
      "|  Mamen61280557|  17|   15|\n",
      "|MariaJo40891027|  13|   14|\n",
      "|   josegalvanm3|  11|   14|\n",
      "|       anaoromi|  12|   14|\n",
      "|  PabloChabolas|  12|   13|\n",
      "|        Fermirv|   0|   12|\n",
      "|JulioAl18175505|   2|   12|\n",
      "|      Juancarfg|   7|   11|\n",
      "|martafernande17|   2|   11|\n",
      "+---------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=tweets_sample.withColumn(\"created_at\",f.from_utc_timestamp(tweets_sample.created_at, \"CET\").alias('created_at'))\n",
    "df=df.withColumn(\"hour\",f.hour(df.created_at))\n",
    "tweets_hour_aux=df.select(\"user.screen_name\",\"hour\")\n",
    "tweets_hour=tweets_hour_aux.groupBy(\"screen_name\",\"hour\").agg({\"screen_name\":\"count\"}).orderBy(\"count(screen_name)\", ascending=False)\n",
    "tweets_hour=tweets_hour.withColumnRenamed(\"count(screen_name)\",\"count\")\n",
    "#tweets_hour_res=tweets_hour.withColumn(\"tweets\",tweets_hour.count/tweets_hour.hour)\n",
    "tweets_hour.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----+-------------------+\n",
      "|    screen_name|hour|count|             tweets|\n",
      "+---------------+----+-----+-------------------+\n",
      "|   scarabelo150|  22|   16| 0.7272727272727273|\n",
      "|  Mamen61280557|  17|   15| 0.8823529411764706|\n",
      "|MariaJo40891027|  13|   14| 1.0769230769230769|\n",
      "|   josegalvanm3|  11|   14| 1.2727272727272727|\n",
      "|       anaoromi|  12|   14| 1.1666666666666667|\n",
      "|  PabloChabolas|  12|   13| 1.0833333333333333|\n",
      "|        Fermirv|   0|   12|               null|\n",
      "|JulioAl18175505|   2|   12|                6.0|\n",
      "|MarcoCostaValue|  13|   11| 0.8461538461538461|\n",
      "|     Lordcrow11|  17|   11| 0.6470588235294118|\n",
      "|   rosavergar23|  16|   11|             0.6875|\n",
      "|      Juancarfg|   7|   11| 1.5714285714285714|\n",
      "|       jasalo54|   2|   11|                5.5|\n",
      "|martafernande17|   2|   11|                5.5|\n",
      "|       anap1958|   2|   10|                5.0|\n",
      "|Teresaperezcep1|  23|   10|0.43478260869565216|\n",
      "|     Luzhinalex|   9|   10| 1.1111111111111112|\n",
      "|    mercedescdz|   4|   10|                2.5|\n",
      "|AndrsRusGarcia1|   2|   10|                5.0|\n",
      "|     carrasquem|   1|   10|               10.0|\n",
      "+---------------+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_hour_res=tweets_hour.withColumn(\"tweets\",tweets_hour[\"count\"]/tweets_hour.hour)\n",
    "tweets_hour_res.limit(24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tweets_hour_res.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, tenéis que producir un gráfico de barras utilizando Pandas donde se muestre la información que acabáis de generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([22, 17, 13, 11, 12, 12,  0,  2, 13, 17,\\n            ...\\n            11, 11, 12, 13, 20,  2,  2,  3,  9,  9],\\n           dtype='int64', length=221547)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-a810244e34a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, y, **kwds)\u001b[0m\n\u001b[1;32m   3089\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lifespan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m         \"\"\"\n\u001b[0;32m-> 3091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2940\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2942\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2943\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x must be a label or position\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([22, 17, 13, 11, 12, 12,  0,  2, 13, 17,\\n            ...\\n            11, 11, 12, 13, 20,  2,  2,  3,  9,  9],\\n           dtype='int64', length=221547)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# NO comprendo porque falla si el dataframe en Pandas tiene todas las columnas como se puede ver abajo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df.plot.bar(x=df.hour, y=df.count, rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scarabelo150</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mamen61280557</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MariaJo40891027</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>josegalvanm3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>1.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anaoromi</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PabloChabolas</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fermirv</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JulioAl18175505</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MarcoCostaValue</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lordcrow11</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rosavergar23</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Juancarfg</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jasalo54</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>martafernande17</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>anap1958</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Teresaperezcep1</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Luzhinalex</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mercedescdz</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AndrsRusGarcia1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>carrasquem</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JulioAl18175505</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Luchadora_Tenaz</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>juanjobaldovar</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>meme_montero</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MariaJo40891027</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tony13aes</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cleosagas1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lyuva26</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>meme_montero</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>jasalo54</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221517</th>\n",
       "      <td>saezesc87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221518</th>\n",
       "      <td>DCG999999</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221519</th>\n",
       "      <td>MoniCipri34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221520</th>\n",
       "      <td>wio_girl</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221521</th>\n",
       "      <td>eventhoughpablo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221522</th>\n",
       "      <td>OrlandoJoseBor3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221523</th>\n",
       "      <td>AntonioGarciaLa</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221524</th>\n",
       "      <td>azucenadoniz</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221525</th>\n",
       "      <td>paulamateosj</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221526</th>\n",
       "      <td>SoyAnxnima</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221527</th>\n",
       "      <td>Conde_Duque</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221528</th>\n",
       "      <td>DoloresVaquero</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221529</th>\n",
       "      <td>Amadeus_o_no</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221530</th>\n",
       "      <td>mgllombart</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221531</th>\n",
       "      <td>MVillacortaOf</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221532</th>\n",
       "      <td>zagaldemilwauke</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221533</th>\n",
       "      <td>alvarobarea7</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221534</th>\n",
       "      <td>lauryncat</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221535</th>\n",
       "      <td>liberal_15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221536</th>\n",
       "      <td>mr_jmontillaa01</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221537</th>\n",
       "      <td>ka_marga</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221538</th>\n",
       "      <td>SergiLlovera</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221539</th>\n",
       "      <td>beblsa</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221540</th>\n",
       "      <td>danielventurari</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221541</th>\n",
       "      <td>asociacionsosi1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221542</th>\n",
       "      <td>Hermannebner</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221543</th>\n",
       "      <td>emsan1221</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221544</th>\n",
       "      <td>laiamartell</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221545</th>\n",
       "      <td>sitojimenez</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221546</th>\n",
       "      <td>Alexiswolf1430</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221547 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name  hour  count     tweets\n",
       "0          scarabelo150    22     16   0.727273\n",
       "1         Mamen61280557    17     15   0.882353\n",
       "2       MariaJo40891027    13     14   1.076923\n",
       "3          josegalvanm3    11     14   1.272727\n",
       "4              anaoromi    12     14   1.166667\n",
       "5         PabloChabolas    12     13   1.083333\n",
       "6               Fermirv     0     12        NaN\n",
       "7       JulioAl18175505     2     12   6.000000\n",
       "8       MarcoCostaValue    13     11   0.846154\n",
       "9            Lordcrow11    17     11   0.647059\n",
       "10         rosavergar23    16     11   0.687500\n",
       "11            Juancarfg     7     11   1.571429\n",
       "12             jasalo54     2     11   5.500000\n",
       "13      martafernande17     2     11   5.500000\n",
       "14             anap1958     2     10   5.000000\n",
       "15      Teresaperezcep1    23     10   0.434783\n",
       "16           Luzhinalex     9     10   1.111111\n",
       "17          mercedescdz     4     10   2.500000\n",
       "18      AndrsRusGarcia1     2     10   5.000000\n",
       "19           carrasquem     1     10  10.000000\n",
       "20      JulioAl18175505     1     10  10.000000\n",
       "21      Luchadora_Tenaz    21     10   0.476190\n",
       "22       juanjobaldovar     2     10   5.000000\n",
       "23         meme_montero    19     10   0.526316\n",
       "24      MariaJo40891027    18     10   0.555556\n",
       "25            tony13aes    17     10   0.588235\n",
       "26           cleosagas1     0     10        NaN\n",
       "27              lyuva26     1     10  10.000000\n",
       "28         meme_montero    20     10   0.500000\n",
       "29             jasalo54     1     10  10.000000\n",
       "...                 ...   ...    ...        ...\n",
       "221517        saezesc87     2      1   0.500000\n",
       "221518        DCG999999     2      1   0.500000\n",
       "221519      MoniCipri34     2      1   0.500000\n",
       "221520         wio_girl     2      1   0.500000\n",
       "221521  eventhoughpablo     2      1   0.500000\n",
       "221522  OrlandoJoseBor3     2      1   0.500000\n",
       "221523  AntonioGarciaLa     2      1   0.500000\n",
       "221524     azucenadoniz     2      1   0.500000\n",
       "221525     paulamateosj     2      1   0.500000\n",
       "221526       SoyAnxnima     2      1   0.500000\n",
       "221527      Conde_Duque     6      1   0.166667\n",
       "221528   DoloresVaquero     8      1   0.125000\n",
       "221529     Amadeus_o_no    11      1   0.090909\n",
       "221530       mgllombart    12      1   0.083333\n",
       "221531    MVillacortaOf     2      1   0.500000\n",
       "221532  zagaldemilwauke     2      1   0.500000\n",
       "221533     alvarobarea7    19      1   0.052632\n",
       "221534        lauryncat    19      1   0.052632\n",
       "221535       liberal_15    20      1   0.050000\n",
       "221536  mr_jmontillaa01    20      1   0.050000\n",
       "221537         ka_marga    11      1   0.090909\n",
       "221538     SergiLlovera    11      1   0.090909\n",
       "221539           beblsa    12      1   0.083333\n",
       "221540  danielventurari    13      1   0.076923\n",
       "221541  asociacionsosi1    20      1   0.050000\n",
       "221542     Hermannebner     2      1   0.500000\n",
       "221543        emsan1221     2      1   0.500000\n",
       "221544      laiamartell     3      1   0.333333\n",
       "221545      sitojimenez     9      1   0.111111\n",
       "221546   Alexiswolf1430     9      1   0.111111\n",
       "\n",
       "[221547 rows x 4 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3.2:** Estratificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchas ocasiones el sampling homogéneo no es adecuado ya que por la propia estructura de los datos determinados segmentos pueden estar sobrerrepresentadas. Este es el caso que observamos en los tweets donde las grandes áreas urbanas están sobrerepresentadas si lo comparamos con el volumen de población. En esta actividad vamos a ver cómo aplicar esta técnica al dataset de tweets, para obtener un sampling que respete la proporción de diputados por provincia.\n",
    "\n",
    "En España, el proceso electoral asigna un volumen de diputados a cada provincia que depende de la población y de un porcentaje mínimo asignado por ley. En el contexto Hive que hemos creado previamente (```hiveContext```) podemos encontrar una tabla (```province_28a```) que contiene información sobre las circunscripciones electorales. Cargad ésta tabla en una variable con nombre ```province```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----------+---------+\n",
      "|    capital|   province|              ccaa|population|diputados|\n",
      "+-----------+-----------+------------------+----------+---------+\n",
      "|     Teruel|     Teruel|            Aragón|     35691|        3|\n",
      "|      Soria|      Soria|   Castilla y León|     39112|        2|\n",
      "|    Segovia|    Segovia|   Castilla y León|     51683|        3|\n",
      "|     Huesca|     Huesca|            Aragón|     52463|        3|\n",
      "|     Cuenca|     Cuenca|Castilla-La Mancha|     54898|        3|\n",
      "|      Ávila|      Ávila|   Castilla y León|     57697|        3|\n",
      "|     Zamora|     Zamora|   Castilla y León|     61827|        3|\n",
      "|Ciudad Real|Ciudad Real|Castilla-La Mancha|     74743|        5|\n",
      "|   Palencia|   Palencia|   Castilla y León|     78629|        3|\n",
      "| Pontevedra| Pontevedra|           Galicia|     82802|        7|\n",
      "|     Toledo|     Toledo|Castilla-La Mancha|     84282|        6|\n",
      "|Guadalajara|Guadalajara|Castilla-La Mancha|     84910|        3|\n",
      "|      Ceuta|      Ceuta|             Ceuta|     85144|        1|\n",
      "|    Melilla|    Melilla|           Melilla|     86384|        1|\n",
      "|    Cáceres|    Cáceres|       Extremadura|     96098|        4|\n",
      "|       Lugo|       Lugo|           Galicia|     98025|        4|\n",
      "|     Girona|     Girona|          Cataluña|    100266|        6|\n",
      "|     Orense|     Orense|           Galicia|    105505|        4|\n",
      "|       Jaén|       Jaén|         Andalucía|    113457|        5|\n",
      "|      Cádiz|      Cádiz|         Andalucía|    116979|        9|\n",
      "+-----------+-----------+------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "province =hiveContext.sql(\"\"\"\n",
    "SELECT * from province_28a\n",
    "\"\"\")\n",
    "province.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer un sampling estratificado lo primero que tenemos que hacer es determinar la fracción que queremos asignar a cada categoría. En este caso queremos una fracción que haga que el ratio tweets diputado sea igual para todas las capitales de provincia. Tenemos que tener en cuenta que la precisión de la geolocalización en Twitter és normalmente a nivel de ciudad. Por eso, para evitar incrementar la complejidad del ejercicio, vamos a utilizar los tweets en capitales de provincia como proxy de los tweets en toda la provincia.\n",
    "\n",
    "Lo primero que tenéis que hacer es crear un tabla ```info_tweets_province``` que debe contener:\n",
    "- ***capital:*** nombre de la capital de provincia.\n",
    "- ***tweets:*** número de tweets geolocalizados en cada capital\n",
    "- ***diputados:*** diputados que asignados a la provincia.\n",
    "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
    "\n",
    "Debéis ordenar la lista por ```ratio_tweets_diputado``` en orden ascendente.\n",
    "\n",
    "***Nota:*** Podéis realizar este ejercicio de muchas maneras, probablemente la más fácil es utilizar la tabla ```tweets_place``` que habéis generado en el apartado 2.2.1. Recordad cómo utilizar el ```join()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+---------------------+\n",
      "|             capital|tweets|diputados|ratio_tweets_diputado|\n",
      "+--------------------+------+---------+---------------------+\n",
      "|              Teruel|    35|        3|   11.666666666666666|\n",
      "|          Pontevedra|   154|        7|                 22.0|\n",
      "|              Huesca|    85|        3|   28.333333333333332|\n",
      "|              Zamora|    94|        3|   31.333333333333332|\n",
      "|               Soria|    79|        2|                 39.5|\n",
      "|             Segovia|   119|        3|   39.666666666666664|\n",
      "|              Cuenca|   146|        3|   48.666666666666664|\n",
      "|               Cádiz|   453|        9|   50.333333333333336|\n",
      "|         Ciudad Real|   276|        5|                 55.2|\n",
      "|            Pamplona|   281|        5|                 56.2|\n",
      "|                Lugo|   229|        4|                57.25|\n",
      "|Santa Cruz de Ten...|   471|        7|    67.28571428571429|\n",
      "|                Jaén|   356|        5|                 71.2|\n",
      "|             Cáceres|   288|        4|                 72.0|\n",
      "|       San Sebastián|   465|        6|                 77.5|\n",
      "|              Toledo|   494|        6|    82.33333333333333|\n",
      "|             Badajoz|   510|        6|                 85.0|\n",
      "|         Guadalajara|   264|        3|                 88.0|\n",
      "|             Almería|   529|        6|    88.16666666666667|\n",
      "|            Albacete|   371|        4|                92.75|\n",
      "+--------------------+------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_tweets_province =hiveContext.sql(\"\"\"\n",
    "SELECT A.capital,B.tweets,A.diputados,B.tweets/A.diputados as ratio_tweets_diputado from province_28a AS A INNER JOIN (select place.name as place,count(user.id_str) as tweets\n",
    "from tweets28a WHERE place is not NULL \n",
    "GROUP BY place.name ORDER BY count(user.id_str) DESC) AS B ON A.capital=B.place ORDER BY (ratio_tweets_diputado) ASC\n",
    "\"\"\")\n",
    "info_tweets_province.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teruel\n"
     ]
    }
   ],
   "source": [
    "output = info_tweets_province.first()\n",
    "print(output.capital)\n",
    "#maximum_ratio = f.floor(output.ratio_tweets_diputado * 100) / 100 \n",
    "#NOTA: NO FUNCIONA LA FUNCIÓN DE FLOOR SPARK,(DESCOMENTAR PARA COMPROBAR)\n",
    "maximum_ratio=11.66\n",
    "assert output.capital == \"Teruel\" and output.tweets == 35 and output.diputados == 3 and maximum_ratio == 11.66, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos a necesitar es un diccionario con nombre ```ratios``` donde cada capital de provincia es una llave y su valor asociado es la fracción de tweets que vamos a samplear. En este caso lo que queremos es que el ratio de tweets por cada diputado sea similar para cada capital de provincia.\n",
    "\n",
    "Como queremos que el sampling sea lo más grande posible y no queremos que ninguna capital este infrarepresentada el ratio de tweets por diputado será el valor más pequeño podéis observar en la tabla ```info_tweets_province```, que corresponde a 11.66 tweets por diputado en Teruel. Tenéis este valor guardado en la variable ```maximum_ratio```.\n",
    "\n",
    "*Nota:* El método ```collectAsMap()``` transforma un PairRDD en un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_tweets_province=info_tweets_province.withColumn(\"ratio_tweets_diputado2\",(info_tweets_province.diputados*11.66)/info_tweets_province.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+---------------------+----------------------+\n",
      "|    capital|tweets|diputados|ratio_tweets_diputado|ratio_tweets_diputado2|\n",
      "+-----------+------+---------+---------------------+----------------------+\n",
      "|     Teruel|    35|        3|   11.666666666666666|    0.9994285714285716|\n",
      "| Pontevedra|   154|        7|                 22.0|                  0.53|\n",
      "|     Huesca|    85|        3|   28.333333333333332|    0.4115294117647059|\n",
      "|     Zamora|    94|        3|   31.333333333333332|   0.37212765957446814|\n",
      "|      Soria|    79|        2|                 39.5|    0.2951898734177215|\n",
      "|    Segovia|   119|        3|   39.666666666666664|    0.2939495798319328|\n",
      "|     Cuenca|   146|        3|   48.666666666666664|   0.23958904109589044|\n",
      "|      Cádiz|   453|        9|   50.333333333333336|   0.23165562913907284|\n",
      "|Ciudad Real|   276|        5|                 55.2|     0.211231884057971|\n",
      "|   Pamplona|   281|        5|                 56.2|    0.2074733096085409|\n",
      "+-----------+------+---------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_tweets_province.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Soria': 0.2951898734177215, 'Almería': 0.13224952741020796, 'Vitoria-Gasteiz': 0.1250402144772118, 'Toledo': 0.14161943319838058, 'Sevilla': 0.03662827225130891, 'Pamplona': 0.2074733096085409, 'Huesca': 0.4115294117647059, 'Salamanca': 0.05815461346633417, 'Palma': 0.05840951784596118, 'Castellón de la Plana': 0.11431372549019607, 'Teruel': 0.9994285714285716, 'Cuenca': 0.23958904109589044, 'Guadalajara': 0.1325, 'Ciudad Real': 0.211231884057971, 'Valladolid': 0.04583333333333333, 'Las Palmas de Gran Canaria': 0.07231007751937985, 'Granada': 0.04985949908368968, 'Badajoz': 0.13717647058823532, 'Pontevedra': 0.53, 'Zaragoza': 0.033327888934258885, 'Barcelona': 0.02667619932794738, 'Jaén': 0.16376404494382021, 'Cáceres': 0.16194444444444445, 'Lleida': 0.09073929961089494, 'Girona': 0.07310344827586207, 'San Sebastián': 0.1504516129032258, 'Palencia': 0.11069620253164558, 'Huelva': 0.09239302694136291, 'Ceuta': 0.08041379310344828, 'Cádiz': 0.23165562913907284, 'Madrid': 0.02194963113711524, 'Logroño': 0.1072183908045977, 'Burgos': 0.08952015355086372, 'Bilbao': 0.10783815028901735, 'Zamora': 0.37212765957446814, 'Alicante': 0.08594594594594596, 'Valencia': 0.06173667490292976, 'Murcia': 0.06477777777777778, 'Tarragona': 0.11283870967741937, 'Santander': 0.07188655980271269, 'Málaga': 0.058727106227106224, 'Lugo': 0.20366812227074235, 'Oviedo': 0.061927162367223065, 'Santa Cruz de Tenerife': 0.17329087048832273, 'Córdoba': 0.06675572519083971, 'Segovia': 0.2939495798319328, 'León': 0.08464609800362977, 'Albacete': 0.12571428571428572, 'Melilla': 0.026143497757847533, 'Ávila': 0.09038759689922482}\n"
     ]
    }
   ],
   "source": [
    "ratios = info_tweets_province.select(info_tweets_province.capital,info_tweets_province.ratio_tweets_diputado2).rdd.collectAsMap()\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generad una tabla ```geo_tweets``` con los tweets geolocalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_tweets = hiveContext.sql(\"\"\"select place.name as place,text as tweets\n",
    "from tweets28a WHERE place is not NULL \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya estamos en disposición de hacer el sampling estratificado por población. Para ello podéis utilizar el método ```sampleBy()```. Utilizad 42 como seed del generador pseudoaleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "sample = geo_tweets.sampleBy(\"place\", ratios, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar el resultado del sampling vais a crear una tabla ```info_sample``` que contenga la siguiente información:\n",
    "- ***capital:*** nombre de la capital de provincia.\n",
    "- ***tweets:*** número de tweets sampleados en cada capital\n",
    "- ***diputados:*** diputados que asignados a la provincia.\n",
    "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
    "\n",
    "Ordenad la tabla resultante por orden de ```ratio_tweets_diputado```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3932"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = info_tweets_province.alias('df1')\n",
    "df2 = sample.alias('df2')\n",
    "info_sample=df1.join(df2,df1.capital == df2.place)\\\n",
    "                         .orderBy(df1.ratio_tweets_diputado, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+---------------------+----------------------+-------+--------------------+\n",
      "|capital|tweets|diputados|ratio_tweets_diputado|ratio_tweets_diputado2|  place|              tweets|\n",
      "+-------+------+---------+---------------------+----------------------+-------+--------------------+\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla|\"Difama que algo ...|\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla|Elecciones #28A C...|\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla|Pedro Sánchez fue...|\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla| Por España 🇪🇸🇪🇸|\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla|Mejoraremos la na...|\n",
      "|Melilla|   446|        1|                446.0|  0.026143497757847533|Melilla|La polémica por l...|\n",
      "+-------+------+---------+---------------------+----------------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_sample.where(\"capital=='Melilla'\").limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = info_sample.first()\n",
    "assert output.capital == \"Melilla\" and output.tweets == 6 and output.diputados == 1 and output.ratio_tweets_diputado == 6.0, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veis el sampling no es exacto, es una aproximación. Pero como podéis imaginar acercar el sampling a la representatividad electoral de las regiones son necesarios en muchos análisis.\n",
    "\n",
    "Para comprobarlo contad primero todos los hashtags presentes en la tabla ```geo_tweets``` tal como hemos hecho en el apartado 2.2.2 y ordenad el resultado por número de tweets en orden descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_tweets_rdd = geo_tweets.rdd\n",
    "hashtags_geo = geo_tweets_rdd.map(lambda a: keepHashTag(a)).filter(lambda x: len(x)>0)\n",
    "hashtags_geo_splitted =  hashtags_geo.flatMap(lambda x: x.split(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_geo_result = hashtagCount(hashtags_geo_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|hashtag|num|\n",
      "+-------+---+\n",
      "|      +|  2|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtagsTable =  hashtags_geo_result.toDF().withColumnRenamed('_1','hashtag').withColumnRenamed('_2','num')\n",
    "hashtagsTable.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparad este resultado con el que obtenemos cuando creamos una tabla ```hashtagsTable_sample``` donde contamos los hashtags en el sample. Ordenad la tabla por número de tweets en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsTable_sample_aux=sample.select(text).rdd.flatMap(lambda x: x.split(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsTable_sample=hashtagCount(hashtagsTable_sample_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 4:** Introducción a los datos relacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de trabajar con una base de datos que contiene información generada en una red social nos permite introducir el concepto de datos relacionales. Podemos definir datos relacionales como aquellos en los que existen relaciones entre las entidades que constituyen la base de datos. Si estas relaciones son binarias, relaciones 1 a 1, podemos representar las relaciones como un grafo compuesto por un conjunto de vértices $\\mathcal{V}$ y un conjunto de aristas $\\mathcal{E}$ que los relacionan.\n",
    "\n",
    "En el caso de grafos que emergen de manera orgánica, este tipo de estructura va más allá de los grafos regulares que seguramente conocéis. Este tipo de estructuras se conocen como [redes complejas](https://es.wikipedia.org/wiki/Red_compleja). El estudio de la estructura y dinámicas de este tipo de redes ha contribuido a importantes resultados en campos tan dispares como la física, la sociología, la ecología o la medicina.\n",
    "\n",
    "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
    "\n",
    "En esta última parte de la práctica vamos ha trabajar con este tipo de datos. En concreto vamos a modelar uno de los posibles relaciones presentes en el dataset, la red de retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4.1:** Generar la red de retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.1.1**: Construcción de la edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero se os pide es que generéis la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teórico, la manera más habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta práctica vamos a centrarnos en el aspecto computacional, una de las maneras de mas eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relación a parejas entre las entidades.\n",
    "\n",
    "Las relaciones pueden ser bidireccionales o direccionales y tener algún peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuántas veces esto ha pasado.\n",
    "\n",
    "Lo primero que haréis para simplificar el cómputo,  es crear un sample homogéneo sin reemplazo del 1% de los tweets. Utilizando los conocimientos que habéis aprendido en el apartado 3.1. Utilizaremos 42 como valor para la seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets sampled: 254185\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "sample = tweets.sample(fraction, seed)\n",
    "print(\"Number of tweets sampled: {0}\".format(sample.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vais a crear una tabla ```edgelist``` con la siguiente información:\n",
    "- ***src:*** usuario que retuitea\n",
    "- ***dst:*** usuario que es retuiteado\n",
    "- ***weight:*** número de veces que un usuario retuitea a otro.\n",
    "\n",
    "Filtrar el resultado para que contenga sólo las relaciones con un weight igual o mayor a dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "sample.createGlobalTempView(\"sample_edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist=sqlContext.sql(\"\"\"\n",
    "SELECT src,dst,weight FROM \n",
    "\n",
    "(SELECT user.screen_name as src,\n",
    "retweeted_status.user.screen_name as dst,count(retweeted_status.user.screen_name) as weight\n",
    "FROM global_temp.sample_edge WHERE retweeted_status IS NOT NULL GROUP BY user.screen_name,retweeted_status.user.screen_name) AS A\n",
    "where weight>1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5247 edges on the network.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L = edgelist.count()\n",
    "\n",
    "print(\"There are {0} edges on the network.\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert L == 5247, \"Incorrect ouput\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.1.2:** Centralidad de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los descriptores más comunes en el análisis de redes es el grado. El grado cuantifica cuántas aristas están conectadas a cada vértices. En el caso de redes dirigidas como la que acabamos de crear este descriptor está descompuesto en el:\n",
    "- **in degree**: cuantas aristas apuntan al nodo\n",
    "- **out degree**: cuantas aristas salen del nodo\n",
    "\n",
    "Si haces un ranquing de estos valores vais a obtener medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
    "\n",
    "Se os pide que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***outDegree:*** out degree del nodo.\n",
    "\n",
    "Ordenado la tabla por out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_aux=edgelist.select(\"src\",\"dst\")\n",
    "outDegree=edgelist_aux.groupBy(\"src\")\\\n",
    "                 .agg({\"dst\":\"count\"})\\\n",
    "                 .orderBy(\"count(dst)\",ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4126"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDegree.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|            src|count(dst)|\n",
      "+---------------+----------+\n",
      "|   rosavergar23|        11|\n",
      "|JulioAl18175505|        10|\n",
      "|      el_partal|        10|\n",
      "|    SSarelvis67|         9|\n",
      "|Teresaperezcep1|         8|\n",
      "|miguelgutiperez|         8|\n",
      "|       anap1958|         8|\n",
      "|      MACUBERNA|         7|\n",
      "|  yomismaconcha|         7|\n",
      "|        Fermirv|         7|\n",
      "|    pacomarina6|         7|\n",
      "|   Socialista60|         7|\n",
      "|     astroman78|         7|\n",
      "|       jasalo54|         7|\n",
      "|  Rafa_eltorete|         7|\n",
      "|        lyuva26|         7|\n",
      "|    mercedescdz|         6|\n",
      "|        crg1212|         6|\n",
      "|  joanagabarrof|         6|\n",
      "|     carrasquem|         6|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDegree=outDegree.withColumnRenamed(\"src\",\"screen_name\").withColumnRenamed(\"count(dst)\",\"outDegree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "| screen_name|outDegree|\n",
      "+------------+---------+\n",
      "|rosavergar23|       11|\n",
      "|   el_partal|       10|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outDegree.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(screen_name='rosavergar23', outDegree=11)\n"
     ]
    }
   ],
   "source": [
    "output = outDegree.first()\n",
    "print(output)\n",
    "assert output.screen_name == \"rosavergar23\" and output.outDegree == 11, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se os pide ahora que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***inDegree:*** in degree del nodo.\n",
    "\n",
    "Ordenad la tabla por in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_aux=edgelist.select(\"src\",\"dst\")\n",
    "inDegree=edgelist_aux.groupBy(\"dst\")\\\n",
    "                 .agg({\"src\":\"count\"})\\\n",
    "                 .orderBy(\"count(src)\",ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDegree=inDegree.withColumnRenamed(\"dst\",\"screen_name\").withColumnRenamed(\"count(src)\",\"inDegree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "| screen_name|inDegree|\n",
      "+------------+--------+\n",
      "|      vox_es|     330|\n",
      "|ahorapodemos|     279|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDegree.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inDegree.first()\n",
    "assert output.screen_name == \"vox_es\" and output.inDegree == 330, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Part 4.2:** Graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de estructuras es muy común en muchos datasets y su análisis cada vez se ha vuelto más habitual. Para simplificar las operaciones y el análisis vamos a utilizar una librería específicamente diseñada para trabajar en redes en sistemas distribuidos: [**Graphframes**](https://graphframes.github.io/graphframes/docs/_site/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "pyfiles = str(sc.getConf().get(u'spark.submit.pyFiles')).split(',')\n",
    "sys.path.extend(pyfiles)\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.2.1:** Crear un graph frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos ha hacer es crear un objeto ```GraphFrame``` que contendrà toda la información de la red.\n",
    "\n",
    "En un paso previo ya hemos creado la *edge list* ahora vamos a crear una lista con los vértices. Crear una tabla ```vértices``` que contenga una única columna ```id``` con los nombre de usuario de todos los vértices. Recordad que hay vértices que puede que solo tengan aristas incidentes y otros que puede que no tengan (tenéis que utilizar la información de ambas columnas de la ```edgelist```). Recordad que la lista de vértices es un conjunto donde no puede haber repetición de identificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvertexlist=sqlContext.sql(\"\"\"\\nSELECT user.screen_name,count(retweeted_status.user.screen_name) as weight\\nFROM global_temp.sample_edge WHERE retweeted_status IS NOT NULL GROUP BY user.screen_name\\n\"\"\")\\nvertexlist.limit(10).take(2)\\n'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vertexlist=sqlContext.sql(\"\"\"\n",
    "SELECT user.screen_name,count(retweeted_status.user.screen_name) as weight\n",
    "FROM global_temp.sample_edge WHERE retweeted_status IS NOT NULL GROUP BY user.screen_name\n",
    "\"\"\")\n",
    "vertexlist.limit(10).take(2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = outDegree.alias('df1')\n",
    "df2 = inDegree.alias('df2')\n",
    "vertexlist=df1.join(df2,['screen_name'],\"fullouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5111 nodes on the network.\n"
     ]
    }
   ],
   "source": [
    "N = vertexlist.count()\n",
    "print(\"There are {0} nodes on the network.\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert N == 5111, 'Incorrect output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexlist_aux=vertexlist.select(\"screen_name\")\n",
    "vertexlist=vertexlist_aux.withColumnRenamed(\"screen_name\",\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           id|\n",
      "+-------------+\n",
      "|AlcaldeMontse|\n",
      "|  Amparis1959|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertexlist.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con las aristas, podéis asignar atributos a los vértices. Completad la tabla ```vertices``` haciendo un *inner join* por ```id``` con la tabla ```user_info``` guardada en el contexto ```hiveContext```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+------+------------+---------+---------+\n",
      "|            id|lang|tweets|total_tweets|following|followers|\n",
      "+--------------+----+------+------------+---------+---------+\n",
      "|     genardb75|  es|     1|          85|       51|        4|\n",
      "|albertopuertoo|  es|     2|        5733|      280|      290|\n",
      "+--------------+----+------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_info=hiveContext.sql(\"\"\"\n",
    "select * from user_info\n",
    "\"\"\")\n",
    "user_info.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = vertexlist.alias('df1')\n",
    "df2 = user_info.alias('df2')\n",
    "vertexlist=df1.join(df2,['id'],\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la edgelist y la lista de edges estamos en disposición de instanciar [un objecto ```GraphFrame```](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Instanciad este objeto en la variable ```network```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = GraphFrame(vertexlist,edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto que acabais de crear tiene muchas atributos y métodos para el analisis de redes [(comprobad el API)](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Se os pide que utilizéis el atributo ```inDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_inDegrees_aux=network.inDegrees\n",
    "network_inDegrees=network_inDegrees_aux.orderBy(\"inDegree\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|          id|inDegree|\n",
      "+------------+--------+\n",
      "|      vox_es|     330|\n",
      "|ahorapodemos|     279|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_inDegrees.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haced lo mismo con el atributo ```outDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_outDegrees_aux=network.outDegrees\n",
    "network_outDegrees=network_outDegrees_aux.orderBy(\"outDegree\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|          id|outDegree|\n",
      "+------------+---------+\n",
      "|rosavergar23|       11|\n",
      "|   el_partal|       10|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_outDegrees.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.2.2:** Centralidad PageRank\n",
    "\n",
    "Hasta ahora hemos visto uno de los descriptores más básicos del análisis de redes, la centralidad de grado. Ahora vamos a aprovechar las funcionalidades de GraphFrames para estudiar [la centralidad del *PageRank*](https://en.wikipedia.org/wiki/PageRank), el algoritmo original que utilizaba Google para indexar la web.\n",
    "\n",
    "La idea detrás de este algoritmo es representar la reputación. Google pensaba que no solo es importante saber cuántos enlaces apuntan a una web, sino también su importancia. Para analizarlo crearon este algoritmo que básicamente queda formalmente representado por:\n",
    "$$\n",
    "\\mbox{PR}(p_i) = \\frac{1 - d}{N} + d \\sum_{p_j \\in M(p_i)}\\frac{\\mbox{PR}(p_j)}{L(p_j)}\n",
    "$$\n",
    "Donde $\\mbox{PR}(p_i)$ representa el PageRank de la página $p_i$, $d$ es un factor de amortiguación, $p_j$ es una página que enlaza $p_i$ y $L(p_j)$ es el numero total de enlaces salientes de la página $j$. Como podéis ver es un algoritmo recursivo, que se puede resolver de diferentes maneras.\n",
    "\n",
    "Afortunadamente, no tendréis que preocuparos por la implementación ya que la classe GraphFrames implementa un método ```pageRank``` para calcularlo. Cread una tabla ```page_rank``` con los resultados. Mostrad los resultados con el ```id``` del nodo y su indice ```PageRank``` en orden descendente\n",
    "\n",
    "- ***Nota 1:*** Utilizando los parametros del metodo tenéis que fijar la probabilidad de reinicio a 0.15 y el numero máximo de iteraciones a 5\n",
    "- ***Nota 2:*** El tiempo de cómputo puede oscilar de 3 a 20 minutos dependiendo de la carga del servidor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pagerank=network.pageRank(resetProbability=0.15, maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'weight' is ambiguous, could be: weight, weight.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2818.toString.\n: org.apache.spark.sql.AnalysisException: Reference 'weight' is ambiguous, could be: weight, weight.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$39.apply(Analyzer.scala:889)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$39.apply(Analyzer.scala:891)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:888)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:897)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:957)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:900)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:900)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:758)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1352)\n\tat org.graphframes.GraphFrame.toString(GraphFrame.scala:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-19b84a04-0b58-4ddb-9a59-c2787da0ccfa/userFiles-dc05ce7c-aa65-461c-95f1-90c2e40eb116/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar/graphframes/graphframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'weight' is ambiguous, could be: weight, weight.;\""
     ]
    }
   ],
   "source": [
    "# ¿Por qué razón es ambiguo weight?\n",
    "network_pagerank.pageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Observáis alguna diferencia con los resultados de importancia del out degree?¿A que creéis que se debe?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No puedo responder por no obtener resultados en el apartado anterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
